{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rf-sero2.ipynb","provenance":[{"file_id":"12Uwy2B49X5Ar3uQJBV_b3zxKP74_ocI-","timestamp":1618186433388}],"collapsed_sections":[],"mount_file_id":"1Kf-6Fi5STeq8fGAYwFDfR0bsegu6hNyJ","authorship_tag":"ABX9TyNQg1a+eaH+AIV8ruieSnNd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5b30cqa5JiET"},"source":["Google Colab specific settings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jvW1gg-bIkwQ","executionInfo":{"status":"ok","timestamp":1618186463281,"user_tz":300,"elapsed":8131,"user":{"displayName":"David Biagini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPXNEuCDO4MZZrcNt32EOIZqfbB4_eLV0HcOFoOw=s64","userId":"18170855662239852923"}},"outputId":"367e9a52-ff12-4665-dbd1-093360a76df3"},"source":["from pathlib import Path\n","root_dir = \"/content/drive/MyDrive/Colab Notebooks/dev/\"\n","base_dir = root_dir + 'nn-sero-pytorch/randomforest/'\n","path = Path(base_dir)\n","NN_dir = root_dir + 'nn-sero-pytorch/'\n","\n","!pip install lime"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting lime\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/86/91a13127d83d793ecb50eb75e716f76e6eda809b6803c5a4ff462339789e/lime-0.2.0.1.tar.gz (275kB)\n","\r\u001b[K     |█▏                              | 10kB 14.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 20.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 16.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 16.2MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81kB 12.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 12.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 235kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 12.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 12.6MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.41.1)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (0.22.2.post1)\n","Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.16.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.0.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n","Building wheels for collected packages: lime\n","  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lime: filename=lime-0.2.0.1-cp37-none-any.whl size=283846 sha256=e19c993ea3b5b9ba71b10a69e659195ec6e67c5390db6dfa48f8c374ee46660b\n","  Stored in directory: /root/.cache/pip/wheels/4c/4f/a5/0bc765457bd41378bf3ce8d17d7495369d6e7ca3b712c60c89\n","Successfully built lime\n","Installing collected packages: lime\n","Successfully installed lime-0.2.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wcVlZt3mIokC"},"source":["from pathlib import Path\n","\n","root_dir = '../'\n","base_dir = root_dir + 'randomforest/'\n","path = Path(base_dir)\n","NN_dir = '../'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eeT9MbRvI7a_","executionInfo":{"status":"ok","timestamp":1618186478698,"user_tz":300,"elapsed":2291,"user":{"displayName":"David Biagini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPXNEuCDO4MZZrcNt32EOIZqfbB4_eLV0HcOFoOw=s64","userId":"18170855662239852923"}}},"source":["import pandas as pd\n","import numpy as np\n","import sys\n","import math\n","import lime\n","import lime.lime_tabular\n","from tqdm import tqdm\n","from sklearn.ensemble import RandomForestClassifier\n","#from sklearn.multioutput import MultiOutputClassifier\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_classification\n","from collections import defaultdict\n","from scipy.stats import spearmanr\n","from scipy.cluster import hierarchy\n","from sklearn.inspection import permutation_importance\n","#from sklearn.model_selection import train_test_split\n","\n","def metrics(print_all='no'):\n","    loci = ['A', 'B', 'C', 'DQB1', 'DRB1']\n","    #loci = ['A']\n","\n","    # function to check if value can be an integer - to eliminate excess characters from serology labels\n","    def checkInt(x):\n","        try:\n","            int(x)\n","            return True\n","        except ValueError:\n","            return False\n","\n","    concordances = {}\n","\n","    for loc in loci:\n","        newDict = {}\n","        simDict = {}\n","        diffDict = {}\n","        oldPredict = {}\n","        newPredict = {}\n","        oldPredFile = Path(NN_dir + \"old-predictions/\" + loc + \".chile\")\n","        newPreds = pd.read_csv(base_dir + \"predictions/\" + loc + \"_predictions.csv\")\n","        newPreds = newPreds.set_index('allele')\n","        newPreds = newPreds.to_dict()\n","        newPredict = newPreds[\"serology\"]\n","        for nKey in newPredict.keys():\n","            adjustMe = newPredict[nKey]\n","            adjustMe = adjustMe.replace('[','')\n","            adjustMe = adjustMe.replace(']','')\n","            adjustMe = adjustMe.replace(' ','')\n","            adjustMe = adjustMe.replace(\"'\",'')\n","            adjustMe = adjustMe.split(',')\n","            newPredict[nKey] = [x.strip('a') for x in adjustMe if checkInt(x)]\n","        with open(oldPredFile, \"r\") as handle:\n","            for line in handle:\n","                if line.find('%') == -1:\n","                    next\n","                else:\n","                    line = line.split()\n","                    if line == []:\n","                        next\n","                    else:\n","                        line[:] = [x for x in line if (x != '[100.00%]')]\n","                        allele = loc + \"*\" + str(line[0][:-1])\n","                        oldPredict[allele] = line[1:]\n","\n","        if loc == 'C':\n","            skipc = ['C*01', 'C*02', 'C*03', 'C*04', 'C*05', 'C*06', 'C*07', 'C*08']\n","            oldPredict = {k:v for k,v in oldPredict.items() if k[:4] in skipc}\n","            newPredict = {k:v for k,v in newPredict.items() if k[:4] in skipc}\n","\n","\n","        for each in oldPredict.keys():\n","            allDict = {}\n","            allDict[\"Allele\"] = each\n","            allDict[\"Old Assignment\"] = oldPredict[each]\n","            if each not in newPredict.keys():\n","                next\n","            else:\n","                allDict[\"New Assignment\"] = newPredict[each]\n","                if set(newPredict[each]) != set(oldPredict[each]):\n","                    diffDict[each] = allDict\n","                elif set(newPredict[each]) == set(oldPredict[each]):\n","                    simDict[each] = allDict\n","        diffFrame = pd.DataFrame.from_dict(diffDict)\n","        diffFrame = diffFrame.transpose()\n","        diffFrame.to_csv(base_dir + \"comparison/\" + loc + \"_compfile.csv\", index=False)\n","        simFrame = pd.DataFrame.from_dict(simDict)\n","        simFrame = simFrame.transpose()\n","        simFrame.to_csv(base_dir + \"comparison/\" + loc + \"_similar.csv\", index=False)\n","        \n","\n","        for allele in newPredict.keys():\n","            allDict = {}\n","            allDict[\"Allele\"] = allele\n","            allDict[\"Serologic Assignment\"] = newPredict[allele]\n","            if allele not in oldPredict.keys():\n","                newDict[allele] = allDict\n","        newFrame = pd.DataFrame.from_dict(simDict)\n","        newFrame = newFrame.transpose()\n","        newFrame.to_csv(base_dir + \"comparison/\" + loc + \"_newsies.csv\", index=False)\n","\n","        simLen = len(simFrame)\n","        diffLen = len(diffFrame)\n","        with open(base_dir + \"comparison/\" + loc + \"_concordance.txt\", \"w+\") as fhandle:\n","            fhandle.write(\"HLA-\" +loc+ \" Similar: \" + str(simLen))\n","            fhandle.write(\"HLA-\" +loc+ \" Different: \" + str(diffLen))\n","            concordance = (simLen / (simLen + diffLen)) * 100\n","            concordances[loc] = concordance\n","            fhandle.write(\"HLA-\" +loc+ \" Concordance: \" + str(concordance) + \"%\")\n","            if print_all == \"yes\":\n","                print(\"HLA-\" +loc+ \" Similar: \" + str(simLen))\n","                print(\"HLA-\" +loc+ \" Different: \" + str(diffLen))\n","                print(\"HLA-\" +loc+ \" Concordance: \" + str(concordance) + \"%\")\n","    return concordances\n","\n","#main(print_all=\"yes\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"8X6XHjx8I_4_","executionInfo":{"status":"ok","timestamp":1618186480918,"user_tz":300,"elapsed":410,"user":{"displayName":"David Biagini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPXNEuCDO4MZZrcNt32EOIZqfbB4_eLV0HcOFoOw=s64","userId":"18170855662239852923"}}},"source":["np.set_printoptions(threshold=sys.maxsize)\n","\n","def one_hot_decode(df):\n","\tdf['serology']=''\n","\n","\tfor col in df.columns:\n","\t\tdf.loc[df[col]==1,'serology'] = df['serology']+col+';'\n","\n","\treturn df\n","\n","def fix_data(uniques, data, loc, iset, ident):\n","    sero = {}\n","    for row in data.itertuples(name='Pandas'):\n","        sero[row.allele] = str(row.serology)\n","        #sero[row[1]] = str(row[-1])\n","  \t\n","    data = data.drop('serology', axis=1)\n","  \n","    for key in sero.keys():\n","        '''\n","    \t    # not applicable for old_sets train/test\n","        if (sero[key].find(';') != -1):\n","            sero[key] = sero[key].replace('a','')\n","            sero[key] = sero[key].split(';')\n","        else:\n","            sero[key] = sero[key].replace('a','')\n","            sero[key] = [sero[key]]\n","        '''\n","  \n","        #for old_sets train/test\n","        sero[key] = sero[key].split(' ')\n","      \n","        for x in sero[key]:\n","            if (x not in uniques):\n","                uniques.append(x)\n","            else:\n","                continue\n","  \n","    uniques = list(map(int, uniques))\n","    uniques.sort()\n","    uniques = list(map(str, uniques))\n","    \n","    for y in uniques:\n","        data[y] = 0\n","  \n","    one_sero = {}\n","    for key in sero.keys():\n","        one_sero[key] = { some_key : (\"1\" if (some_key in sero[key]) else \"0\")\n","  \t  \t                    for some_key in uniques }\n","    one_df = pd.DataFrame.from_dict(one_sero)\n","    one_df = one_df.transpose()\n","    one_df.index.name = \"allele\"\n","    data = data.set_index('allele')\n","    data.update(one_df, overwrite=True)\n","    data.to_csv(base_dir + 'randfor/'+iset+'/'+loc+'_'+ident+'.csv', index=True)\n","    return data, uniques\n","  "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gMuQNEezJCnX","outputId":"8486716c-1b6e-4ab9-9cbf-e217463b5194"},"source":["#RSEED = 0\n","\n","pre_concord = metrics()\n","\n","loci = [\"A\", \"B\", \"C\", \"DQB1\", \"DRB1\"]\n","print(\"Predicting...\")\n","for loc in tqdm(loci):\n","    uniques = []\n","    print(loc)\n","    features = pd.read_csv(base_dir + \"training/\" + loc + \"_train.csv\")\n","    #features['serology'] = features['serology'].apply(lambda x: x.replace('a','').replace(';',' '))\n","    features, sers = fix_data(uniques, features,loc,iset='training',ident='train')\n","    vfeatures = pd.read_csv(base_dir + \"training/\" + loc + \"_validation.csv\")\n","    #vfeatures['serology'] = vfeatures['serology'].apply(lambda x: x.replace('a','').replace(';',' '))\n","    vfeatures, vsers = fix_data(uniques, vfeatures,loc,iset='training',ident='validation')\n","    test = pd.read_csv(base_dir + \"testing/\" + loc + \"_test.csv\")\n","    test = test.drop('serology', axis=1)\n","    test.to_csv(base_dir + 'randfor/testing/'+loc+'_test.csv', index=True)\n","  \n","    features = features.append(vfeatures)\n","    labels = np.array(features[sers])\n","    features = features.drop(sers, axis=1)\n","    features = features.reset_index()\n","    indices = features[\"allele\"]\n","    indices = list(indices)\n","    features = features.drop('allele', axis=1)\n","    feature_list = list(features.columns)\n","    n_features = len(feature_list)\n","    maxfeat = int(math.sqrt(n_features))\n","  \n","    features = np.array(features)\n","    labels[labels!=labels]='0'\n","    features[features!=features]='0'\n","    features = features.astype(int)\n","    labels = labels.astype(int)\n","  \n","    test_idcs = test['allele']\n","    test = test.drop('allele', axis=1)\n","    #print(test.head(100))\n","    test_list = list(test.columns)\n","    test = np.array(test)\n","    test[test!=test]='0'\n","    test = test.astype(int)\n","    ind_labels = [str(x) for x in sers]\n","\n","    all_predictions = []\n","    for idx in range(0,len(ind_labels)):\n","        ilabels = labels[:,idx]\n","        forest = RandomForestClassifier(n_estimators=500, bootstrap=True, max_features=maxfeat, n_jobs=-1)\n","        forest.fit(features,ilabels)\n","        predictions = forest.predict(test)\n","        all_predictions.append(predictions)\n","\n","        ''' \n","        # Feature Importance\n","\n","        importances = forest.feature_importances_\n","\n","        std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n","                    axis=0)\n","        indices = np.argsort(importances)[::-1]\n","        nz_importances = importances[importances!=0]\n","        nz_indices = indices[:len(nz_importances)]\n","\n","        with open(base_dir+'feat_importance/'+loc+'-'+ind_labels[idx]+'_rank.txt', 'w+') as handle:\n","            # Print the feature ranking\n","            handle.write(\"Feature ranking: \\n\")\n","            for x in range(features.shape[1]):\n","                handle.write(\"%d. feature %s (%f)\\n\" % (x + 1, feature_list[indices[x]], importances[indices[x]]))\n","\n","        with open(base_dir+'feat_importance/'+loc+'-'+ind_labels[idx]+'_nzrank.txt', 'w+') as handle:\n","            # Print the feature ranking\n","            handle.write(\"Feature ranking: \\n\")\n","            for y in range(len(nz_indices)):\n","                handle.write(\"%d. feature %s (%f)\\n\" % (y + 1, feature_list[nz_indices[y]], importances[nz_indices[y]]))\n","\n","        # Plot the impurity-based feature importances of the forest\n","        f = plt.figure()\n","        plt.title(\"Feature importances\")\n","        #plt.bar(range(features.shape[1]), importances[indices],\n","        #        color=\"r\", yerr=std[indices], align=\"center\")\n","        #\n","        #plt.xticks(range(features.shape[1]), indices)\n","        #plt.xlim([-1, features.shape[1]])\n","        \n","        plt.bar(range(len(nz_indices)), importances[nz_indices],\n","                color=\"r\", yerr=std[nz_indices], align=\"center\")\n","        nzlabs = [feature_list[nz_indices[i]] for i in range(0, len(nz_indices))] \n","        plt.xticks(range(len(nz_indices)), nzlabs, rotation='vertical', fontsize=3) \n","        plt.xlim([-1, len(nz_indices)])\n","        plt.tight_layout()\n","        \n","        \n","        #plt.show()\n","        #f = plt.figure()\n","        f.savefig(base_dir+'feat_importance/'+loc+'-'+ind_labels[idx]+\"_nz.pdf\", bbox_inches='tight')\n","\n","        plt.bar(range(features.shape[1]), importances[indices],\n","                color=\"r\", yerr=std[indices], align=\"center\")  \n","        labs = [feature_list[indices[j]] for j in range(0, len(indices))]\n","        plt.xticks(range(features.shape[1]), labs, rotation='vertical', fontsize=3)\n","        plt.xlim([-1, features.shape[1]])\n","        plt.tight_layout()\n","        f.savefig(base_dir+'feat_importance/'+loc+'-'+ind_labels[idx]+\".pdf\", bbox_inches='tight')\n","        plt.clf()\n","\n","        '''\n","        # Permutation Importance\n","        result = permutation_importance(forest, features, ilabels, n_repeats=2, random_state=0, n_jobs=-1)\n","        perm_sorted_idx = result.importances_mean.argsort()\n","        print(perm_sorted_idx)\n","\n","        with open(base_dir+'perm_importance/'+loc+'-'+ind_labels[idx]+'_rank.txt', 'w+') as handle:\n","            # Print the feature ranking\n","            handle.write(\"Feature ranking: \\n\")\n","            for x in range(features.shape[1]):\n","                handle.write(\"%d. feature %s (%f)\\n\" % (x + 1, feature_list[indices[x]], importances[indices[x]]))\n","\n","        tree_importance_sorted_idx = np.argsort(forest.feature_importances_)\n","        tree_indices = np.arange(0, len(forest.feature_importances_)) + 0.5\n","\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n","        ax1.barh(tree_indices,\n","                forest.feature_importances_[tree_importance_sorted_idx], height=0.7)\n","        ax1.set_yticklabels(data.feature_names[tree_importance_sorted_idx])\n","        ax1.set_yticks(tree_indices)\n","        ax1.set_ylim((0, len(forest.feature_importances_)))\n","        ax2.boxplot(result.importances[perm_sorted_idx].T, vert=False,\n","                    labels=data.feature_names[perm_sorted_idx])\n","        fig.tight_layout()\n","        f = plt.figure()\n","        f.savefig(base_dir+'perm_importance/'+loc+'-'+ind_labels[idx]+\".pdf\", bbox_inches='tight')\n","        #plt.show()\n","        \n","\n","    all_predictions = np.asarray(all_predictions)\n","    all_predictions = np.transpose(all_predictions)\n","\n","    #explainer = lime.lime_tabular.LimeTabularExplainer(features,feature_names=feature_list,class_names=ind_labels,kernel_width=5)\n","    #for rowexp in range(0,2):\n","    #  exp = explainer.explain_instance(test[rowexp], forest.predict_proba, num_features=maxfeat)\n","    #  exp.show_in_notebook(show_table=True)\n","  \n","    #preds_output = pd.DataFrame(predictions, index=test_idcs, columns=ind_labels)\n","    preds_output = pd.DataFrame(all_predictions, index=test_idcs, columns=ind_labels)\n","    preds_output = one_hot_decode(preds_output)\n","    preds_output = preds_output.drop(ind_labels, axis=1)\n","    preds_output.index.name = 'allele'\n","    preds_output = preds_output.apply(lambda x: str((x['serology'].split(';'))[:-1]), result_type='broadcast', axis=1)\n","    preds_output.to_csv(base_dir + 'predictions/'+loc+'_predictions.csv', index=True)\n","\n","print(\"Done.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","\n","  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Predicting...\n","A\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HP2SpEXdJFEy","executionInfo":{"status":"ok","timestamp":1618176851517,"user_tz":300,"elapsed":1896,"user":{"displayName":"David Biagini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPXNEuCDO4MZZrcNt32EOIZqfbB4_eLV0HcOFoOw=s64","userId":"18170855662239852923"}},"outputId":"0ca8413d-d2c6-4197-c2fa-016238c2d7b3"},"source":["post_concord = metrics()\n","\n","for loc in loci:\n","\tprint(loc + \" Concordance:\\t\\t\\t\\t\" + str(post_concord[loc])[:5] + \"%\")\n","\tchange = post_concord[loc] - pre_concord[loc]\n","\tprint(\"% Change:\\t\\t\\t\\t\" + str(change)[:5] + \"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["A Concordance:\t\t\t\t94.61%\n","% Change:\t\t\t\t0.236%\n","B Concordance:\t\t\t\t83.11%\n","% Change:\t\t\t\t0.176%\n","C Concordance:\t\t\t\t86.85%\n","% Change:\t\t\t\t12.77%\n","DQB1 Concordance:\t\t\t\t86.95%\n","% Change:\t\t\t\t-0.72%\n","DRB1 Concordance:\t\t\t\t90.53%\n","% Change:\t\t\t\t0.697%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WuvH-_EVJH2q","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1618177015061,"user_tz":300,"elapsed":10714,"user":{"displayName":"David Biagini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPXNEuCDO4MZZrcNt32EOIZqfbB4_eLV0HcOFoOw=s64","userId":"18170855662239852923"}},"outputId":"558ec88c-7d4b-4a30-b360-d05f0651d956"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Feature ranking:\n","1. feature 225 (0.087509)\n","2. feature 55 (0.071417)\n","3. feature 584 (0.063105)\n","4. feature 71 (0.042562)\n","5. feature 184 (0.041923)\n","6. feature 239 (0.039151)\n","7. feature 183 (0.033770)\n","8. feature 543 (0.032522)\n","9. feature 11 (0.030986)\n","10. feature 238 (0.030400)\n","11. feature 587 (0.024533)\n","12. feature 2 (0.023875)\n","13. feature 204 (0.023839)\n","14. feature 235 (0.022173)\n","15. feature 6 (0.022122)\n","16. feature 7 (0.021203)\n","17. feature 575 (0.020658)\n","18. feature 34 (0.020283)\n","19. feature 203 (0.020165)\n","20. feature 9 (0.020163)\n","21. feature 569 (0.020147)\n","22. feature 10 (0.018319)\n","23. feature 5 (0.017632)\n","24. feature 298 (0.014211)\n","25. feature 544 (0.012845)\n","26. feature 3 (0.012806)\n","27. feature 29 (0.011824)\n","28. feature 255 (0.011484)\n","29. feature 736 (0.011369)\n","30. feature 732 (0.010966)\n","31. feature 610 (0.007844)\n","32. feature 576 (0.007692)\n","33. feature 0 (0.007330)\n","34. feature 1 (0.006858)\n","35. feature 456 (0.006699)\n","36. feature 657 (0.006298)\n","37. feature 43 (0.006285)\n","38. feature 448 (0.006044)\n","39. feature 247 (0.005922)\n","40. feature 58 (0.005917)\n","41. feature 228 (0.005810)\n","42. feature 488 (0.005257)\n","43. feature 231 (0.004668)\n","44. feature 79 (0.004664)\n","45. feature 261 (0.004446)\n","46. feature 301 (0.004325)\n","47. feature 53 (0.004233)\n","48. feature 382 (0.003954)\n","49. feature 645 (0.003843)\n","50. feature 47 (0.003761)\n","51. feature 73 (0.002937)\n","52. feature 376 (0.002913)\n","53. feature 63 (0.002753)\n","54. feature 105 (0.002706)\n","55. feature 541 (0.002594)\n","56. feature 726 (0.002528)\n","57. feature 206 (0.002432)\n","58. feature 82 (0.002407)\n","59. feature 296 (0.002383)\n","60. feature 68 (0.001947)\n","61. feature 303 (0.001704)\n","62. feature 485 (0.001646)\n","63. feature 174 (0.001627)\n","64. feature 655 (0.001614)\n","65. feature 308 (0.001614)\n","66. feature 78 (0.001532)\n","67. feature 615 (0.001471)\n","68. feature 31 (0.001417)\n","69. feature 586 (0.001358)\n","70. feature 229 (0.001350)\n","71. feature 80 (0.001336)\n","72. feature 718 (0.001154)\n","73. feature 59 (0.001093)\n","74. feature 602 (0.001060)\n","75. feature 221 (0.000968)\n","76. feature 641 (0.000966)\n","77. feature 56 (0.000877)\n","78. feature 232 (0.000541)\n","79. feature 51 (0.000488)\n","80. feature 258 (0.000479)\n","81. feature 39 (0.000450)\n","82. feature 583 (0.000426)\n","83. feature 190 (0.000426)\n","84. feature 293 (0.000378)\n","85. feature 18 (0.000318)\n","86. feature 319 (0.000265)\n","87. feature 98 (0.000219)\n","88. feature 767 (0.000188)\n","89. feature 83 (0.000186)\n","90. feature 808 (0.000164)\n","91. feature 222 (0.000161)\n","92. feature 763 (0.000155)\n","93. feature 454 (0.000154)\n","94. feature 458 (0.000102)\n","95. feature 72 (0.000099)\n","96. feature 791 (0.000082)\n","97. feature 593 (0.000077)\n","98. feature 460 (0.000073)\n","99. feature 603 (0.000057)\n","100. feature 175 (0.000045)\n","101. feature 447 (0.000040)\n","102. feature 742 (0.000039)\n","103. feature 466 (0.000038)\n","104. feature 611 (0.000034)\n","105. feature 613 (0.000021)\n","106. feature 750 (0.000020)\n","107. feature 12 (0.000016)\n","108. feature 616 (0.000012)\n","109. feature 21 (0.000012)\n","110. feature 794 (0.000010)\n","111. feature 481 (0.000006)\n","112. feature 786 (0.000006)\n","113. feature 48 (0.000003)\n","114. feature 799 (0.000003)\n","115. feature 295 (0.000001)\n","116. feature 24 (0.000001)\n","117. feature 739 (0.000001)\n","118. feature 802 (0.000001)\n","119. feature 400 (0.000000)\n","120. feature 535 (0.000000)\n","121. feature 324 (0.000000)\n","122. feature 361 (0.000000)\n","123. feature 362 (0.000000)\n","124. feature 227 (0.000000)\n","125. feature 374 (0.000000)\n","126. feature 259 (0.000000)\n","127. feature 256 (0.000000)\n","128. feature 265 (0.000000)\n","129. feature 270 (0.000000)\n","130. feature 269 (0.000000)\n","131. feature 268 (0.000000)\n","132. feature 267 (0.000000)\n","133. feature 223 (0.000000)\n","134. feature 266 (0.000000)\n","135. feature 264 (0.000000)\n","136. feature 257 (0.000000)\n","137. feature 263 (0.000000)\n","138. feature 262 (0.000000)\n","139. feature 224 (0.000000)\n","140. feature 260 (0.000000)\n","141. feature 254 (0.000000)\n","142. feature 226 (0.000000)\n","143. feature 363 (0.000000)\n","144. feature 364 (0.000000)\n","145. feature 253 (0.000000)\n","146. feature 241 (0.000000)\n","147. feature 372 (0.000000)\n","148. feature 375 (0.000000)\n","149. feature 233 (0.000000)\n","150. feature 234 (0.000000)\n","151. feature 371 (0.000000)\n","152. feature 236 (0.000000)\n","153. feature 237 (0.000000)\n","154. feature 370 (0.000000)\n","155. feature 377 (0.000000)\n","156. feature 230 (0.000000)\n","157. feature 369 (0.000000)\n","158. feature 368 (0.000000)\n","159. feature 240 (0.000000)\n","160. feature 242 (0.000000)\n","161. feature 252 (0.000000)\n","162. feature 272 (0.000000)\n","163. feature 243 (0.000000)\n","164. feature 244 (0.000000)\n","165. feature 245 (0.000000)\n","166. feature 246 (0.000000)\n","167. feature 367 (0.000000)\n","168. feature 366 (0.000000)\n","169. feature 365 (0.000000)\n","170. feature 248 (0.000000)\n","171. feature 373 (0.000000)\n","172. feature 249 (0.000000)\n","173. feature 250 (0.000000)\n","174. feature 251 (0.000000)\n","175. feature 271 (0.000000)\n","176. feature 344 (0.000000)\n","177. feature 273 (0.000000)\n","178. feature 320 (0.000000)\n","179. feature 328 (0.000000)\n","180. feature 327 (0.000000)\n","181. feature 326 (0.000000)\n","182. feature 325 (0.000000)\n","183. feature 323 (0.000000)\n","184. feature 322 (0.000000)\n","185. feature 321 (0.000000)\n","186. feature 318 (0.000000)\n","187. feature 330 (0.000000)\n","188. feature 317 (0.000000)\n","189. feature 316 (0.000000)\n","190. feature 315 (0.000000)\n","191. feature 351 (0.000000)\n","192. feature 314 (0.000000)\n","193. feature 313 (0.000000)\n","194. feature 312 (0.000000)\n","195. feature 329 (0.000000)\n","196. feature 331 (0.000000)\n","197. feature 310 (0.000000)\n","198. feature 347 (0.000000)\n","199. feature 343 (0.000000)\n","200. feature 346 (0.000000)\n","201. feature 342 (0.000000)\n","202. feature 341 (0.000000)\n","203. feature 340 (0.000000)\n","204. feature 339 (0.000000)\n","205. feature 338 (0.000000)\n","206. feature 337 (0.000000)\n","207. feature 332 (0.000000)\n","208. feature 336 (0.000000)\n","209. feature 335 (0.000000)\n","210. feature 348 (0.000000)\n","211. feature 334 (0.000000)\n","212. feature 349 (0.000000)\n","213. feature 333 (0.000000)\n","214. feature 350 (0.000000)\n","215. feature 311 (0.000000)\n","216. feature 220 (0.000000)\n","217. feature 274 (0.000000)\n","218. feature 282 (0.000000)\n","219. feature 287 (0.000000)\n","220. feature 286 (0.000000)\n","221. feature 285 (0.000000)\n","222. feature 358 (0.000000)\n","223. feature 284 (0.000000)\n","224. feature 283 (0.000000)\n","225. feature 359 (0.000000)\n","226. feature 345 (0.000000)\n","227. feature 289 (0.000000)\n","228. feature 281 (0.000000)\n","229. feature 280 (0.000000)\n","230. feature 279 (0.000000)\n","231. feature 278 (0.000000)\n","232. feature 277 (0.000000)\n","233. feature 276 (0.000000)\n","234. feature 275 (0.000000)\n","235. feature 288 (0.000000)\n","236. feature 290 (0.000000)\n","237. feature 309 (0.000000)\n","238. feature 302 (0.000000)\n","239. feature 352 (0.000000)\n","240. feature 353 (0.000000)\n","241. feature 307 (0.000000)\n","242. feature 306 (0.000000)\n","243. feature 305 (0.000000)\n","244. feature 304 (0.000000)\n","245. feature 354 (0.000000)\n","246. feature 355 (0.000000)\n","247. feature 291 (0.000000)\n","248. feature 300 (0.000000)\n","249. feature 299 (0.000000)\n","250. feature 356 (0.000000)\n","251. feature 297 (0.000000)\n","252. feature 357 (0.000000)\n","253. feature 294 (0.000000)\n","254. feature 292 (0.000000)\n","255. feature 360 (0.000000)\n","256. feature 188 (0.000000)\n","257. feature 219 (0.000000)\n","258. feature 90 (0.000000)\n","259. feature 99 (0.000000)\n","260. feature 97 (0.000000)\n","261. feature 96 (0.000000)\n","262. feature 95 (0.000000)\n","263. feature 94 (0.000000)\n","264. feature 93 (0.000000)\n","265. feature 92 (0.000000)\n","266. feature 91 (0.000000)\n","267. feature 89 (0.000000)\n","268. feature 101 (0.000000)\n","269. feature 88 (0.000000)\n","270. feature 87 (0.000000)\n","271. feature 86 (0.000000)\n","272. feature 85 (0.000000)\n","273. feature 84 (0.000000)\n","274. feature 81 (0.000000)\n","275. feature 77 (0.000000)\n","276. feature 76 (0.000000)\n","277. feature 100 (0.000000)\n","278. feature 102 (0.000000)\n","279. feature 74 (0.000000)\n","280. feature 114 (0.000000)\n","281. feature 122 (0.000000)\n","282. feature 121 (0.000000)\n","283. feature 120 (0.000000)\n","284. feature 119 (0.000000)\n","285. feature 118 (0.000000)\n","286. feature 117 (0.000000)\n","287. feature 116 (0.000000)\n","288. feature 115 (0.000000)\n","289. feature 113 (0.000000)\n","290. feature 103 (0.000000)\n","291. feature 112 (0.000000)\n","292. feature 111 (0.000000)\n","293. feature 110 (0.000000)\n","294. feature 109 (0.000000)\n","295. feature 108 (0.000000)\n","296. feature 107 (0.000000)\n","297. feature 106 (0.000000)\n","298. feature 104 (0.000000)\n","299. feature 75 (0.000000)\n","300. feature 70 (0.000000)\n","301. feature 218 (0.000000)\n","302. feature 22 (0.000000)\n","303. feature 33 (0.000000)\n","304. feature 32 (0.000000)\n","305. feature 30 (0.000000)\n","306. feature 28 (0.000000)\n","307. feature 27 (0.000000)\n","308. feature 26 (0.000000)\n","309. feature 25 (0.000000)\n","310. feature 23 (0.000000)\n","311. feature 20 (0.000000)\n","312. feature 36 (0.000000)\n","313. feature 19 (0.000000)\n","314. feature 17 (0.000000)\n","315. feature 16 (0.000000)\n","316. feature 15 (0.000000)\n","317. feature 14 (0.000000)\n","318. feature 13 (0.000000)\n","319. feature 8 (0.000000)\n","320. feature 4 (0.000000)\n","321. feature 35 (0.000000)\n","322. feature 37 (0.000000)\n","323. feature 69 (0.000000)\n","324. feature 54 (0.000000)\n","325. feature 67 (0.000000)\n","326. feature 66 (0.000000)\n","327. feature 65 (0.000000)\n","328. feature 64 (0.000000)\n","329. feature 62 (0.000000)\n","330. feature 61 (0.000000)\n","331. feature 60 (0.000000)\n","332. feature 57 (0.000000)\n","333. feature 52 (0.000000)\n","334. feature 38 (0.000000)\n","335. feature 50 (0.000000)\n","336. feature 49 (0.000000)\n","337. feature 46 (0.000000)\n","338. feature 45 (0.000000)\n","339. feature 44 (0.000000)\n","340. feature 42 (0.000000)\n","341. feature 41 (0.000000)\n","342. feature 40 (0.000000)\n","343. feature 123 (0.000000)\n","344. feature 124 (0.000000)\n","345. feature 125 (0.000000)\n","346. feature 181 (0.000000)\n","347. feature 192 (0.000000)\n","348. feature 191 (0.000000)\n","349. feature 189 (0.000000)\n","350. feature 379 (0.000000)\n","351. feature 187 (0.000000)\n","352. feature 186 (0.000000)\n","353. feature 185 (0.000000)\n","354. feature 182 (0.000000)\n","355. feature 180 (0.000000)\n","356. feature 194 (0.000000)\n","357. feature 179 (0.000000)\n","358. feature 178 (0.000000)\n","359. feature 177 (0.000000)\n","360. feature 176 (0.000000)\n","361. feature 173 (0.000000)\n","362. feature 172 (0.000000)\n","363. feature 171 (0.000000)\n","364. feature 170 (0.000000)\n","365. feature 193 (0.000000)\n","366. feature 195 (0.000000)\n","367. feature 126 (0.000000)\n","368. feature 209 (0.000000)\n","369. feature 217 (0.000000)\n","370. feature 216 (0.000000)\n","371. feature 215 (0.000000)\n","372. feature 214 (0.000000)\n","373. feature 213 (0.000000)\n","374. feature 212 (0.000000)\n","375. feature 211 (0.000000)\n","376. feature 210 (0.000000)\n","377. feature 208 (0.000000)\n","378. feature 196 (0.000000)\n","379. feature 207 (0.000000)\n","380. feature 205 (0.000000)\n","381. feature 202 (0.000000)\n","382. feature 201 (0.000000)\n","383. feature 200 (0.000000)\n","384. feature 199 (0.000000)\n","385. feature 198 (0.000000)\n","386. feature 197 (0.000000)\n","387. feature 169 (0.000000)\n","388. feature 168 (0.000000)\n","389. feature 167 (0.000000)\n","390. feature 136 (0.000000)\n","391. feature 144 (0.000000)\n","392. feature 143 (0.000000)\n","393. feature 142 (0.000000)\n","394. feature 141 (0.000000)\n","395. feature 140 (0.000000)\n","396. feature 139 (0.000000)\n","397. feature 138 (0.000000)\n","398. feature 137 (0.000000)\n","399. feature 135 (0.000000)\n","400. feature 166 (0.000000)\n","401. feature 134 (0.000000)\n","402. feature 133 (0.000000)\n","403. feature 132 (0.000000)\n","404. feature 131 (0.000000)\n","405. feature 130 (0.000000)\n","406. feature 129 (0.000000)\n","407. feature 128 (0.000000)\n","408. feature 127 (0.000000)\n","409. feature 145 (0.000000)\n","410. feature 146 (0.000000)\n","411. feature 147 (0.000000)\n","412. feature 148 (0.000000)\n","413. feature 165 (0.000000)\n","414. feature 164 (0.000000)\n","415. feature 163 (0.000000)\n","416. feature 162 (0.000000)\n","417. feature 161 (0.000000)\n","418. feature 160 (0.000000)\n","419. feature 159 (0.000000)\n","420. feature 158 (0.000000)\n","421. feature 157 (0.000000)\n","422. feature 156 (0.000000)\n","423. feature 155 (0.000000)\n","424. feature 154 (0.000000)\n","425. feature 153 (0.000000)\n","426. feature 152 (0.000000)\n","427. feature 151 (0.000000)\n","428. feature 150 (0.000000)\n","429. feature 149 (0.000000)\n","430. feature 378 (0.000000)\n","431. feature 810 (0.000000)\n","432. feature 380 (0.000000)\n","433. feature 665 (0.000000)\n","434. feature 674 (0.000000)\n","435. feature 673 (0.000000)\n","436. feature 672 (0.000000)\n","437. feature 671 (0.000000)\n","438. feature 670 (0.000000)\n","439. feature 669 (0.000000)\n","440. feature 668 (0.000000)\n","441. feature 667 (0.000000)\n","442. feature 666 (0.000000)\n","443. feature 664 (0.000000)\n","444. feature 676 (0.000000)\n","445. feature 663 (0.000000)\n","446. feature 662 (0.000000)\n","447. feature 661 (0.000000)\n","448. feature 660 (0.000000)\n","449. feature 659 (0.000000)\n","450. feature 658 (0.000000)\n","451. feature 656 (0.000000)\n","452. feature 654 (0.000000)\n","453. feature 653 (0.000000)\n","454. feature 675 (0.000000)\n","455. feature 677 (0.000000)\n","456. feature 651 (0.000000)\n","457. feature 689 (0.000000)\n","458. feature 698 (0.000000)\n","459. feature 697 (0.000000)\n","460. feature 696 (0.000000)\n","461. feature 695 (0.000000)\n","462. feature 694 (0.000000)\n","463. feature 693 (0.000000)\n","464. feature 692 (0.000000)\n","465. feature 691 (0.000000)\n","466. feature 690 (0.000000)\n","467. feature 688 (0.000000)\n","468. feature 678 (0.000000)\n","469. feature 687 (0.000000)\n","470. feature 686 (0.000000)\n","471. feature 685 (0.000000)\n","472. feature 684 (0.000000)\n","473. feature 683 (0.000000)\n","474. feature 682 (0.000000)\n","475. feature 681 (0.000000)\n","476. feature 680 (0.000000)\n","477. feature 679 (0.000000)\n","478. feature 652 (0.000000)\n","479. feature 650 (0.000000)\n","480. feature 381 (0.000000)\n","481. feature 608 (0.000000)\n","482. feature 622 (0.000000)\n","483. feature 621 (0.000000)\n","484. feature 620 (0.000000)\n","485. feature 619 (0.000000)\n","486. feature 618 (0.000000)\n","487. feature 617 (0.000000)\n","488. feature 614 (0.000000)\n","489. feature 612 (0.000000)\n","490. feature 609 (0.000000)\n","491. feature 607 (0.000000)\n","492. feature 624 (0.000000)\n","493. feature 606 (0.000000)\n","494. feature 605 (0.000000)\n","495. feature 604 (0.000000)\n","496. feature 601 (0.000000)\n","497. feature 600 (0.000000)\n","498. feature 599 (0.000000)\n","499. feature 598 (0.000000)\n","500. feature 597 (0.000000)\n","501. feature 596 (0.000000)\n","502. feature 623 (0.000000)\n","503. feature 625 (0.000000)\n","504. feature 649 (0.000000)\n","505. feature 637 (0.000000)\n","506. feature 648 (0.000000)\n","507. feature 647 (0.000000)\n","508. feature 646 (0.000000)\n","509. feature 644 (0.000000)\n","510. feature 643 (0.000000)\n","511. feature 642 (0.000000)\n","512. feature 640 (0.000000)\n","513. feature 639 (0.000000)\n","514. feature 638 (0.000000)\n","515. feature 636 (0.000000)\n","516. feature 626 (0.000000)\n","517. feature 635 (0.000000)\n","518. feature 634 (0.000000)\n","519. feature 633 (0.000000)\n","520. feature 632 (0.000000)\n","521. feature 631 (0.000000)\n","522. feature 630 (0.000000)\n","523. feature 629 (0.000000)\n","524. feature 628 (0.000000)\n","525. feature 627 (0.000000)\n","526. feature 699 (0.000000)\n","527. feature 700 (0.000000)\n","528. feature 701 (0.000000)\n","529. feature 769 (0.000000)\n","530. feature 778 (0.000000)\n","531. feature 777 (0.000000)\n","532. feature 776 (0.000000)\n","533. feature 775 (0.000000)\n","534. feature 774 (0.000000)\n","535. feature 773 (0.000000)\n","536. feature 772 (0.000000)\n","537. feature 771 (0.000000)\n","538. feature 770 (0.000000)\n","539. feature 768 (0.000000)\n","540. feature 780 (0.000000)\n","541. feature 766 (0.000000)\n","542. feature 765 (0.000000)\n","543. feature 764 (0.000000)\n","544. feature 762 (0.000000)\n","545. feature 761 (0.000000)\n","546. feature 760 (0.000000)\n","547. feature 759 (0.000000)\n","548. feature 758 (0.000000)\n","549. feature 757 (0.000000)\n","550. feature 779 (0.000000)\n","551. feature 781 (0.000000)\n","552. feature 702 (0.000000)\n","553. feature 796 (0.000000)\n","554. feature 807 (0.000000)\n","555. feature 806 (0.000000)\n","556. feature 805 (0.000000)\n","557. feature 804 (0.000000)\n","558. feature 803 (0.000000)\n","559. feature 801 (0.000000)\n","560. feature 800 (0.000000)\n","561. feature 798 (0.000000)\n","562. feature 797 (0.000000)\n","563. feature 795 (0.000000)\n","564. feature 782 (0.000000)\n","565. feature 793 (0.000000)\n","566. feature 792 (0.000000)\n","567. feature 790 (0.000000)\n","568. feature 789 (0.000000)\n","569. feature 788 (0.000000)\n","570. feature 787 (0.000000)\n","571. feature 785 (0.000000)\n","572. feature 784 (0.000000)\n","573. feature 783 (0.000000)\n","574. feature 756 (0.000000)\n","575. feature 755 (0.000000)\n","576. feature 754 (0.000000)\n","577. feature 713 (0.000000)\n","578. feature 723 (0.000000)\n","579. feature 722 (0.000000)\n","580. feature 721 (0.000000)\n","581. feature 720 (0.000000)\n","582. feature 719 (0.000000)\n","583. feature 717 (0.000000)\n","584. feature 716 (0.000000)\n","585. feature 715 (0.000000)\n","586. feature 714 (0.000000)\n","587. feature 712 (0.000000)\n","588. feature 753 (0.000000)\n","589. feature 711 (0.000000)\n","590. feature 710 (0.000000)\n","591. feature 709 (0.000000)\n","592. feature 708 (0.000000)\n","593. feature 707 (0.000000)\n","594. feature 706 (0.000000)\n","595. feature 705 (0.000000)\n","596. feature 704 (0.000000)\n","597. feature 703 (0.000000)\n","598. feature 724 (0.000000)\n","599. feature 725 (0.000000)\n","600. feature 727 (0.000000)\n","601. feature 728 (0.000000)\n","602. feature 752 (0.000000)\n","603. feature 751 (0.000000)\n","604. feature 749 (0.000000)\n","605. feature 748 (0.000000)\n","606. feature 747 (0.000000)\n","607. feature 746 (0.000000)\n","608. feature 745 (0.000000)\n","609. feature 744 (0.000000)\n","610. feature 743 (0.000000)\n","611. feature 741 (0.000000)\n","612. feature 740 (0.000000)\n","613. feature 738 (0.000000)\n","614. feature 737 (0.000000)\n","615. feature 735 (0.000000)\n","616. feature 734 (0.000000)\n","617. feature 733 (0.000000)\n","618. feature 731 (0.000000)\n","619. feature 730 (0.000000)\n","620. feature 729 (0.000000)\n","621. feature 595 (0.000000)\n","622. feature 594 (0.000000)\n","623. feature 592 (0.000000)\n","624. feature 441 (0.000000)\n","625. feature 452 (0.000000)\n","626. feature 451 (0.000000)\n","627. feature 450 (0.000000)\n","628. feature 449 (0.000000)\n","629. feature 446 (0.000000)\n","630. feature 445 (0.000000)\n","631. feature 444 (0.000000)\n","632. feature 443 (0.000000)\n","633. feature 442 (0.000000)\n","634. feature 440 (0.000000)\n","635. feature 455 (0.000000)\n","636. feature 439 (0.000000)\n","637. feature 438 (0.000000)\n","638. feature 437 (0.000000)\n","639. feature 436 (0.000000)\n","640. feature 435 (0.000000)\n","641. feature 434 (0.000000)\n","642. feature 433 (0.000000)\n","643. feature 432 (0.000000)\n","644. feature 431 (0.000000)\n","645. feature 453 (0.000000)\n","646. feature 457 (0.000000)\n","647. feature 484 (0.000000)\n","648. feature 472 (0.000000)\n","649. feature 482 (0.000000)\n","650. feature 480 (0.000000)\n","651. feature 479 (0.000000)\n","652. feature 478 (0.000000)\n","653. feature 477 (0.000000)\n","654. feature 476 (0.000000)\n","655. feature 475 (0.000000)\n","656. feature 474 (0.000000)\n","657. feature 473 (0.000000)\n","658. feature 471 (0.000000)\n","659. feature 459 (0.000000)\n","660. feature 470 (0.000000)\n","661. feature 469 (0.000000)\n","662. feature 468 (0.000000)\n","663. feature 467 (0.000000)\n","664. feature 465 (0.000000)\n","665. feature 464 (0.000000)\n","666. feature 463 (0.000000)\n","667. feature 462 (0.000000)\n","668. feature 461 (0.000000)\n","669. feature 430 (0.000000)\n","670. feature 429 (0.000000)\n","671. feature 428 (0.000000)\n","672. feature 393 (0.000000)\n","673. feature 403 (0.000000)\n","674. feature 402 (0.000000)\n","675. feature 401 (0.000000)\n","676. feature 399 (0.000000)\n","677. feature 398 (0.000000)\n","678. feature 397 (0.000000)\n","679. feature 396 (0.000000)\n","680. feature 395 (0.000000)\n","681. feature 394 (0.000000)\n","682. feature 392 (0.000000)\n","683. feature 427 (0.000000)\n","684. feature 391 (0.000000)\n","685. feature 390 (0.000000)\n","686. feature 389 (0.000000)\n","687. feature 388 (0.000000)\n","688. feature 387 (0.000000)\n","689. feature 386 (0.000000)\n","690. feature 385 (0.000000)\n","691. feature 384 (0.000000)\n","692. feature 383 (0.000000)\n","693. feature 404 (0.000000)\n","694. feature 809 (0.000000)\n","695. feature 406 (0.000000)\n","696. feature 407 (0.000000)\n","697. feature 426 (0.000000)\n","698. feature 425 (0.000000)\n","699. feature 424 (0.000000)\n","700. feature 423 (0.000000)\n","701. feature 422 (0.000000)\n","702. feature 421 (0.000000)\n","703. feature 420 (0.000000)\n","704. feature 419 (0.000000)\n","705. feature 418 (0.000000)\n","706. feature 417 (0.000000)\n","707. feature 416 (0.000000)\n","708. feature 415 (0.000000)\n","709. feature 414 (0.000000)\n","710. feature 413 (0.000000)\n","711. feature 412 (0.000000)\n","712. feature 411 (0.000000)\n","713. feature 410 (0.000000)\n","714. feature 409 (0.000000)\n","715. feature 408 (0.000000)\n","716. feature 483 (0.000000)\n","717. feature 486 (0.000000)\n","718. feature 591 (0.000000)\n","719. feature 550 (0.000000)\n","720. feature 559 (0.000000)\n","721. feature 558 (0.000000)\n","722. feature 557 (0.000000)\n","723. feature 556 (0.000000)\n","724. feature 555 (0.000000)\n","725. feature 554 (0.000000)\n","726. feature 553 (0.000000)\n","727. feature 552 (0.000000)\n","728. feature 551 (0.000000)\n","729. feature 549 (0.000000)\n","730. feature 561 (0.000000)\n","731. feature 548 (0.000000)\n","732. feature 547 (0.000000)\n","733. feature 546 (0.000000)\n","734. feature 545 (0.000000)\n","735. feature 542 (0.000000)\n","736. feature 540 (0.000000)\n","737. feature 539 (0.000000)\n","738. feature 538 (0.000000)\n","739. feature 537 (0.000000)\n","740. feature 560 (0.000000)\n","741. feature 562 (0.000000)\n","742. feature 487 (0.000000)\n","743. feature 577 (0.000000)\n","744. feature 590 (0.000000)\n","745. feature 589 (0.000000)\n","746. feature 588 (0.000000)\n","747. feature 585 (0.000000)\n","748. feature 582 (0.000000)\n","749. feature 581 (0.000000)\n","750. feature 580 (0.000000)\n","751. feature 579 (0.000000)\n","752. feature 578 (0.000000)\n","753. feature 574 (0.000000)\n","754. feature 563 (0.000000)\n","755. feature 573 (0.000000)\n","756. feature 572 (0.000000)\n","757. feature 571 (0.000000)\n","758. feature 570 (0.000000)\n","759. feature 568 (0.000000)\n","760. feature 567 (0.000000)\n","761. feature 566 (0.000000)\n","762. feature 565 (0.000000)\n","763. feature 564 (0.000000)\n","764. feature 536 (0.000000)\n","765. feature 534 (0.000000)\n","766. feature 533 (0.000000)\n","767. feature 499 (0.000000)\n","768. feature 508 (0.000000)\n","769. feature 507 (0.000000)\n","770. feature 506 (0.000000)\n","771. feature 505 (0.000000)\n","772. feature 504 (0.000000)\n","773. feature 503 (0.000000)\n","774. feature 502 (0.000000)\n","775. feature 501 (0.000000)\n","776. feature 500 (0.000000)\n","777. feature 498 (0.000000)\n","778. feature 532 (0.000000)\n","779. feature 497 (0.000000)\n","780. feature 496 (0.000000)\n","781. feature 495 (0.000000)\n","782. feature 494 (0.000000)\n","783. feature 493 (0.000000)\n","784. feature 492 (0.000000)\n","785. feature 491 (0.000000)\n","786. feature 490 (0.000000)\n","787. feature 489 (0.000000)\n","788. feature 509 (0.000000)\n","789. feature 510 (0.000000)\n","790. feature 511 (0.000000)\n","791. feature 512 (0.000000)\n","792. feature 531 (0.000000)\n","793. feature 530 (0.000000)\n","794. feature 529 (0.000000)\n","795. feature 528 (0.000000)\n","796. feature 527 (0.000000)\n","797. feature 526 (0.000000)\n","798. feature 525 (0.000000)\n","799. feature 524 (0.000000)\n","800. feature 523 (0.000000)\n","801. feature 522 (0.000000)\n","802. feature 521 (0.000000)\n","803. feature 520 (0.000000)\n","804. feature 519 (0.000000)\n","805. feature 518 (0.000000)\n","806. feature 517 (0.000000)\n","807. feature 516 (0.000000)\n","808. feature 515 (0.000000)\n","809. feature 514 (0.000000)\n","810. feature 513 (0.000000)\n","811. feature 405 (0.000000)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZxoNKm4rOxI","executionInfo":{"status":"ok","timestamp":1618174482917,"user_tz":300,"elapsed":7731,"user":{"displayName":"David Biagini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPXNEuCDO4MZZrcNt32EOIZqfbB4_eLV0HcOFoOw=s64","userId":"18170855662239852923"}},"outputId":"2328a1c1-923f-46ef-a47f-be350ae49759"},"source":["import pandas as pd\n","import numpy as np\n","\n","loci = ['A', 'B', 'C', 'DQB1', 'DRB1']\n","summary = {}\n","\n","# dict of dicts to store splits of broad specificities\n","broad_split = {\n","    \"A\" : {\n","        \"9\" : [\"23\", \"24\"],\n","        \"10\" : [\"25\", \"26\", \"34\", \"66\"],\n","        \"19\" : [\"29\", \"30\", \"31\", \"32\", \"33\", \"74\"],\n","        \"28\" : [\"68\", \"69\"]\n","    },\n","    \"B\" : {\n","        \"5\" : [\"51\", \"52\"],\n","        \"12\" : [\"44\", \"45\"],\n","        \"14\" : [\"64\", \"65\"],\n","        \"15\" : [\"62\", \"63\", \"75\", \"76\", \"77\"],\n","        \"16\" : [\"38\", \"39\"],\n","        \"17\" : [\"57\", \"58\"],\n","        \"21\" : [\"49\", \"50\"],\n","        \"22\" : [\"54\", \"55\", \"56\"],\n","        \"40\" : [\"60\", \"61\"],\n","        \"70\" : [\"71\", \"72\"]\n","    },\n","    \"C\" : {\n","        \"3\" : [\"9\", \"10\"]\n","    },\n","    \"DQB1\" : {\n","        \"1\" : [\"5\", \"6\"],\n","        \"3\" : [\"7\", \"8\", \"9\"]\n","    },\n","    \"DRB1\" : {\n","        \"2\" : [\"15\", \"16\"],\n","        \"3\" : [\"17\", \"18\"],\n","        \"5\" : [\"11\", \"12\"],\n","        \"6\" : [\"13\", \"14\"]\n","    }\n","}\n","\n","# dict of dict to store broad specificity for each split\n","split_broad = {}\n","\n","for alphakey in broad_split.keys():\n","    sb = {}\n","    \n","    for betakey in broad_split[alphakey].keys():\n","        for value in broad_split[alphakey][betakey]:\n","            sb[value] = betakey\n","    \n","    split_broad[alphakey] = sb\n","\n","# function to check if value can be an integer - to eliminate excess characters from serology labels\n","def checkInt(x):\n","    try:\n","        int(x)\n","        return True\n","    except ValueError:\n","        return False\n","\n","# function to eliminate any serological assignments with under a 95% likelihood\n","def chance(x, line):\n","    if (line[x].find(\"%\") != -1):\n","        x = float(line[x][:-1])\n","        if 51 <= x:\n","            test = True\n","        else:\n","            test = False\n","    else:\n","        test = False\n","    return test\n","\n","# function to generate dataframes to contain SNNS predictions\n","def SNNS_preds(loci=loci):\n","    for loc in loci:\n","        oldPredict = {}\n","        oldPredFile = NN_dir + \"old-predictions/\" + loc + \".chile\"\n","        with open(oldPredFile, \"r\") as handle:\n","            for line in handle:\n","                if line.find('%') != -1:\n","                    line = line.split()\n","                    if line != []:\n","                        line[:] = [x for x in line if x != '[100.00%]']\n","                        allele = loc + \"*\" + str(line[0][:-1])\n","                        oldPredict[allele] = ' '.join(line[1:])\n","                else:\n","                    next\n","\n","        opseries = pd.Series(oldPredict, name=\"serology\")\n","        opseries.index.name = \"allele\"\n","        opseries.reset_index()\n","        opseries.to_csv(NN_dir+\"old-predictions/\"+loc+\"_predictions.csv\", line_terminator='\\n')\n","    return\n","\n","# function to measure concordance between old SNNS and new ML models\n","def concordance(loci=loci):\n","    for loc in loci:\n","        comparison = open(NN_dir+\"comparison/\" + loc + \"_compfile.txt\", \"w+\")\n","        newsies = open(NN_dir+\"comparison/\" + loc + \"_newsies.txt\", \"w+\")\n","        similarities = open(NN_dir+\"comparison/\" + loc + \"_similar.txt\", \"w+\")\n","        oldPredict = {}\n","        newPredict = {}\n","        oldPredFile = NN_dir+\"old-predictions/\" + loc + \".chile\"\n","        newPreds = pd.read_csv(NN_dir+\"predictions/\" + loc + \"_predictions.csv\")\n","        newPreds = newPreds.set_index('allele')\n","        newPreds = newPreds.to_dict()\n","        newPredict = newPreds[\"serology\"]\n","        for nKey in newPredict.keys():\n","            adjustMe = str(newPredict[nKey])\n","            adjustMe = adjustMe.replace('[','')\n","            adjustMe = adjustMe.replace(']','')\n","            adjustMe = adjustMe.replace('a','')\n","            adjustMe = adjustMe.replace(\"'\",'')\n","            adjustMe = adjustMe.split(' ')\n","            newPredict[nKey] = [x.strip('a') for x in adjustMe if checkInt(x)]\n","        with open(oldPredFile, \"r\") as handle:\n","            for line in handle:\n","                if line.find('%') == -1:\n","                    next\n","                else:\n","                    line = handle.readline()\n","                    line = line.split()\n","                    if line == []:\n","                        next\n","                    else:\n","                        line[:] = [x for x in line if x != '[100.00%]']\n","                        allele = loc + \"*\" + str(line[0][:-1])\n","                        oldPredict[allele] = line[1:]\n","\n","        for each in oldPredict.keys():\n","            if each not in newPredict.keys():\n","                next\n","            elif set(newPredict[each]) != set(oldPredict[each]):\n","                comparison.write(\"Different: \" + str(each) + \"\\n\")\n","                comparison.write(\"Old Serologic Assignment: \" + str(oldPredict[each]) + \"\\n\")\n","                comparison.write(\"New Serologic Assignment: \" + str(newPredict[each]) + \"\\n\")\n","            elif set(newPredict[each]) == set(oldPredict[each]):\n","                similarities.write(\"Same: \" + str(each) + \"\\n\")\n","                similarities.write(\"Old Serologic Assignment: \" + str(oldPredict[each]) + \"\\n\")\n","                similarities.write(\"New Serologic Assignment: \" + str(newPredict[each]) + \"\\n\")\n","        comparison.close()\n","        similarities.close()\n","\n","        for allele in newPredict.keys():\n","            if allele not in oldPredict.keys():\n","                newsies.write(\"NEW: \" + str(allele) + \"\\n\")\n","                newsies.write(\"Serologic Assignment: \" + str(newPredict[allele]) + \"\\n\")\n","        newsies.close()\n","    \n","    return\n"," \n","def summary_table(loci=loci, summary=summary):\n","    for locus in loci:\n","        summary_data = {}\n","        trn_set = pd.read_csv('training/' + locus + '_train.csv')\n","        val_set = pd.read_csv('training/' + locus + '_validation.csv')\n","        tst_set = pd.read_csv('testing/' + locus + '_test.csv')\n","\n","        old_trn_set = pd.read_csv('old_sets/train/' + locus + '_train.csv')\n","        old_val_set = pd.read_csv('old_sets/train/' + locus + '_validation.csv')\n","        old_tst_set = pd.read_csv('old_sets/test/' + locus + '_test.csv')\n","\n","        trnlen = float(len(trn_set))\n","        vallen = float(len(val_set))\n","        tstlen = float(len(tst_set))\n","        polyAA = float(len(trn_set.iloc[0])) - 1\n","        oldtrnlen = float(len(old_trn_set))\n","        oldvallen = float(len(old_val_set))\n","        oldtstlen = float(len(old_tst_set))\n","        oldpolyAA = float(len(old_trn_set.iloc[0])) - 1\n","\n","        summary_data['Number of Training Alleles'] = trnlen\n","        summary_data['R-SNNS Number of Training Alleles'] = oldtrnlen\n","        summary_data['Difference in Training Set'] = trnlen - oldtrnlen\n","        summary_data['Percent (%) Growth in Training Set'] = ((trnlen - oldtrnlen)/oldtrnlen) * 100\n","        summary_data['Number of Validation Alleles'] = vallen\n","        summary_data['R-SNNS Number of Validation Alleles'] = oldvallen\n","        summary_data['Difference in Validation Set'] = vallen - oldvallen\n","        summary_data['Percent (%) Growth in Validation Set'] = ((vallen - oldvallen)/oldvallen) * 100\n","        summary_data['Number of Testing Alleles'] = tstlen\n","        summary_data['R-SNNS Number of Testing Alleles'] = oldtstlen\n","        summary_data['Difference in Testing Set'] = tstlen - oldtstlen\n","        summary_data['Percent (%) Growth in Testing Set'] = ((tstlen - oldtstlen)/oldtstlen) * 100\n","        summary_data['Number of Polymorphisms'] = polyAA\n","        summary_data['R-SNNS Number of Polymorphisms'] = oldpolyAA\n","        summary_data['Difference in Polymorphisms'] = polyAA - oldpolyAA\n","        summary_data['Percent (%) Growth in Polymorphisms'] = ((polyAA - oldpolyAA)/oldpolyAA) * 100\n","        summary[locus] = summary_data\n","        \n","    sum_df = pd.DataFrame(data=summary)\n","\n","    sum_df.to_csv(NN_dir+'comparison/summary.csv', index=True)\n","    \n","    return\n","\n","def evaluate(loc, p_allele, relser, right, wrong, partial, close, bad, broad_split=broad_split, split_broad=split_broad):\n","    p_ser = str(p_allele.serology).replace(\"'\",'').replace('[','').replace(']','').replace('\"','').replace(',','')\n","    p_ser = set(p_ser.split(' '))\n","    \n","    if p_allele.allele in relser.index:\n","        newser = set(relser.loc[p_allele.allele].serology.split(' '))\n","        if p_ser == newser:\n","            right.append(p_allele.allele)\n","        elif p_ser != newser:\n","            wrong.append(p_allele.allele)\n","            if any(w in newser for w in p_ser):\n","                partial.append(p_allele.allele)\n","            else:\n","                switch1 = \"no\"\n","                for oldval in p_ser:\n","                    if (oldval in list(broad_split[loc].keys())) or (oldval in list(split_broad[loc].keys())):\n","                        if oldval in list(broad_split[loc].keys()):\n","                            if any(x in newser for x in broad_split[loc][oldval]):\n","                                close.append(p_allele.allele)\n","                                switch1 = \"yes\"\n","                        elif oldval in list(split_broad[loc].keys()):\n","                            if any(y in newser for y in split_broad[loc][oldval]):\n","                                close.append(p_allele.allele)\n","                                switch1 = \"yes\"\n","                if switch1 == \"no\":\n","                    bad.append(p_allele.allele)\n","\n","        return right, wrong, partial, close, bad\n","    \n","    else:\n","        \n","        return right, wrong, partial, close, bad\n","\n","def met_pct(datalist, right, wrong, partial, close, bad):\n","    n_alleles = len(datalist)\n","    n_r = len(right)\n","    n_w = len(wrong)\n","    n_p = len(partial)\n","    n_c = len(close)\n","    n_b = len(bad)\n","\n","    p_r = (n_r / n_alleles) * 100\n","    p_w = (n_w / n_alleles) * 100\n","    p_p = (n_p / n_alleles) * 100\n","    p_c = (n_c / n_alleles) * 100\n","    p_b = (n_b / n_alleles) * 100\n","\n","    p_dict = {\n","        \"All Calls Correct\" : p_r,\n","        \"Incorrect\" : p_w,\n","        \"At Least One Correct Call\" : p_p,\n","        #\"Close\" : p_c,\n","        \"All Calls Incorrect\" : p_b,\n","    }\n","\n","    return p_dict\n","\n","def accuracy(loc, dataframe, relser):\n","    right = []\n","    wrong = []\n","    partial = []\n","    close = []\n","    bad = []\n","\n","    for all in dataframe.iloc:\n","        # FIXME - A*23:19Q does not appear in rel_dna_ser (A*23:19N instead)\n","        # FIXME - B*07:44 does not appear in rel_dna_ser (B*07:44N instead)\n","        # FIXME - B*08:06 does not appear in rel_dna_ser at all\n","        # FIXME - B*49:15 does not appear in rel_dna_ser at all\n","        # FIXME - C*03:23 does not appear in rel_dna_ser (C*03:23N instead)\n","        # FIXME - C*03:99 does not appear in rel_dna_ser at all\n","        # FIXME - C*05:02 does not appear in rel_dna_ser at all\n","        # FIXME - C*07:226 does not appear in rel_dna_ser (C*07:226Q instead)\n","        if all.allele in [\"A*23:19Q\", \"B*07:44\", \"B*08:06\", \"B*49:15\", \"C*03:23\", \"C*03:99\", \"C*05:02\", \"C*07:226\"]:\n","            continue\n","        \n","        right, wrong, partial, close, bad = evaluate(loc, all, relser, right, wrong, partial, close, bad)\n","\n","    df = relser[relser.index.isin(dataframe.allele)]\n","\n","    met_dict = met_pct(df, right, wrong, partial, close, bad)\n","\n","    return met_dict\n","\n","\n","def check_acc_all(loci=loci):\n","    mets = {\n","        \"Old NN\" : {},\n","        \"New NN\" : {},\n","        \"Random Forest\" : {},\n","    }\n","\n","    for loc in loci:    \n","        old_nn_preds = pd.read_csv(NN_dir+\"old-predictions/\"+loc+\"_predictions.csv\", dtype=str)\n","        new_nn_preds = pd.read_csv(NN_dir+\"predictions/\"+loc+\"_predictions.csv\", dtype=str)\n","        new_nn_preds = new_nn_preds[new_nn_preds.allele.isin(old_nn_preds.allele)]\n","        rf_preds = pd.read_csv(NN_dir+\"randomforest/predictions/\"+loc+\"_predictions.csv\", dtype=str)\n","        #rf_preds = rf_preds[rf_preds.allele.isin(rf_preds.allele)]\n","        relser = pd.read_csv(NN_dir+\"ser/\"+loc+\"_ser.csv\", dtype=str)\n","        relser = relser.set_index('allele')\n","        relser = relser.dropna()\n","\n","        mets[\"Old NN\"][loc] = accuracy(loc, old_nn_preds, relser)\n","        mets[\"New NN\"][loc] = accuracy(loc, new_nn_preds, relser)\n","        mets[\"Random Forest\"][loc] = accuracy(loc, rf_preds, relser)\n","\n","    return mets\n","\n","def compare_acc(mets, opt1, opt2, loci=loci):\n","    c_dict = {}\n","    for loc in loci:\n","        l_dict = {}\n","        r1 = mets[opt1][loc]['All Calls Correct']\n","        w1 = mets[opt1][loc]['Incorrect']\n","        p1 = mets[opt1][loc]['At Least One Correct Call']\n","        #c1 = mets[opt1][loc]['Close']\n","        b1 = mets[opt1][loc]['All Calls Incorrect']\n","        r2 = mets[opt2][loc]['All Calls Correct']\n","        w2 = mets[opt2][loc]['Incorrect']\n","        p2 = mets[opt2][loc]['At Least One Correct Call']\n","        #c2 = mets[opt2][loc]['Close']\n","        b2 = mets[opt2][loc]['All Calls Incorrect']\n","\n","        l_dict['All Calls Correct'] = r2-r1\n","        l_dict['Incorrect'] = w2-w1\n","        l_dict['At Least One Correct Call'] = p2-p1\n","        #l_dict['Close'] = c2-c1\n","        l_dict['All Calls Incorrect'] = b2-b1\n","        c_dict[loc] = l_dict\n","\n","    return c_dict\n","\n","def compare_acc_all(mets):\n","    cond1 = \"New_vs_Old_NN\"\n","    cond2 = \"Random_Forest_vs_Old_NN\"\n","    cond3 = \"Random_Forest_vs_New_NN\"\n","\n","    opt1 = \"Old NN\"\n","    opt2 = \"New NN\"\n","    opt3 = \"Random Forest\"\n","\n","    comp_dict = {\n","        cond1 : {},\n","        cond2 : {},\n","        cond3 : {},\n","    }\n","\n","    comp_dict[cond1] = compare_acc(mets, opt1, opt2)\n","    cframe1 = pd.DataFrame.from_dict(comp_dict[cond1])\n","    cframe1.to_csv(NN_dir+'comparison/'+cond1+'.csv', index=True)\n","    comp_dict[cond2] = compare_acc(mets, opt1, opt3)\n","    cframe2 = pd.DataFrame.from_dict(comp_dict[cond2])\n","    cframe2.to_csv(NN_dir+'comparison/'+cond2+'.csv', index=True)\n","    comp_dict[cond3] = compare_acc(mets, opt2, opt3)\n","    cframe3 = pd.DataFrame.from_dict(comp_dict[cond3])\n","    cframe3.to_csv(NN_dir+'comparison/'+cond3+'.csv', index=True)\n","\n","    return comp_dict\n","\n","concordance()\n","mets = check_acc_all()\n","mframe1 = pd.DataFrame.from_dict(mets['Old NN'])\n","mframe1.to_csv(NN_dir+'comparison/OldNN_mets.csv', index=True)\n","mframe2 = pd.DataFrame.from_dict(mets['New NN'])\n","mframe2.to_csv(NN_dir+'comparison/NewNN_mets.csv', index=True)\n","mframe3 = pd.DataFrame.from_dict(mets['Random Forest'])\n","mframe3.to_csv(NN_dir+'comparison/RF_mets.csv', index=True)\n","print(\"Random Forest:\")\n","print(mframe3.to_string())\n","print(\"RSNNS:\")\n","print(mframe1.to_string())\n","comp_dict = compare_acc_all(mets)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                   A          B          C       DQB1       DRB1\n","All Calls Correct          81.159420  85.751979  88.709677  85.714286  90.555556\n","Incorrect                  18.840580  14.248021  11.290323  14.285714   9.444444\n","At Least One Correct Call  12.560386   7.387863   9.677419  10.714286   6.111111\n","All Calls Incorrect         6.280193   6.860158   1.612903   3.571429   3.333333\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wI0ho2Ru1sFy","executionInfo":{"status":"ok","timestamp":1618176603299,"user_tz":300,"elapsed":227,"user":{"displayName":"David Biagini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPXNEuCDO4MZZrcNt32EOIZqfbB4_eLV0HcOFoOw=s64","userId":"18170855662239852923"}},"outputId":"e9368035-7e35-4238-facc-45766a0400db"},"source":["print(\"Random Forest:\")\n","print(mframe3.to_string())\n","print('\\n\\n')\n","print(\"RSNNS:\")\n","print(mframe1.to_string())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Random Forest:\n","                                   A          B          C       DQB1       DRB1\n","All Calls Correct          81.159420  85.751979  88.709677  85.714286  90.555556\n","Incorrect                  18.840580  14.248021  11.290323  14.285714   9.444444\n","At Least One Correct Call  12.560386   7.387863   9.677419  10.714286   6.111111\n","All Calls Incorrect         6.280193   6.860158   1.612903   3.571429   3.333333\n","\n","\n","\n","RSNNS:\n","                                   A          B          C       DQB1       DRB1\n","All Calls Correct          84.343434  89.487871  85.000000  85.714286  90.449438\n","Incorrect                  15.656566  10.512129  15.000000  14.285714   9.550562\n","At Least One Correct Call  12.121212   5.929919  13.333333  10.714286   6.179775\n","All Calls Incorrect         3.030303   4.582210   1.666667   3.571429   3.370787\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aorQDVg-JJ1L"},"source":["# Previous version with MultiOutputClassifier\n","    '''\n","    forest = RandomForestClassifier(n_estimators=500, bootstrap=True, max_features=maxfeat, n_jobs=-1)\n","    multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)\n","    multi_target_forest.fit(features,labels)\n","    predictions = multi_target_forest.predict(test)\n","    print(predictions)\n","    '''"],"execution_count":null,"outputs":[]}]}